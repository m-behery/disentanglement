{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d964c8-18d1-4181-b1c7-f407d142e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import re\n",
    "from typing import final\n",
    "import torch\n",
    "import torchaudio\n",
    "from collections import defaultdict\n",
    "from IPython.display import Audio, display\n",
    "from random import randint\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d526ead-3b3c-4c26-aba2-f38cc82d37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(record):\n",
    "    info     = '\\n'.join(record['raw_text'])\n",
    "    patterns = { \n",
    "        'speaker'    : r'Filename:\\s*([^/]+)',\n",
    "        'accent'     : r'Accent:\\s*([^\\n]+)', \n",
    "        'sex'        : r'Gender:\\s*([^\\n]+)',\n",
    "        'spoken_text': r'Text:\\s*([^\\n]+)',\n",
    "    }\n",
    "    extract_pattern = lambda pattern: re.search(pattern, info).group(1).strip()\n",
    "    metadata = { \n",
    "        key: extract_pattern(patterns[key]) for key in patterns\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "def get_audio(record):\n",
    "    audio = record['audio'].get_all_samples()\n",
    "    samples = audio.data.mean(0)\n",
    "    samplerate = audio.sample_rate\n",
    "    return samples, samplerate\n",
    "\n",
    "def get_audio_attrs(record):\n",
    "    metadata            = get_metadata(record)\n",
    "    dataset, samplerate = get_audio(record)\n",
    "    return metadata, dataset, samplerate\n",
    "\n",
    "# ----\n",
    "\n",
    "def get_audio_groups(metadata, dataset):\n",
    "    \n",
    "    key_components = ['accent', 'spoken_text']\n",
    "    \n",
    "    audio_groups = defaultdict(list)\n",
    "    for idx, audio in enumerate(metadata):\n",
    "        key = tuple(audio[x] for x in key_components)\n",
    "        audio_groups[key].append(idx)\n",
    "    \n",
    "    accents, spoken_texts = map(set, zip(*audio_groups.keys()))\n",
    "    \n",
    "    print(f'''\n",
    "    Original Recordings:\n",
    "    \n",
    "    Total Number of Recordings:      {len(cmu)}\n",
    "    Number of Accents:               {len(accents)}\n",
    "    Number of Spoken Texts:          {len(spoken_texts)}\n",
    "    Number of Accented Spoken Texts: {len(audio_groups)}\n",
    "    ''')\n",
    "\n",
    "    return audio_groups\n",
    "\n",
    "# ---\n",
    "\n",
    "def get_matching_pairs(metadata, dataset, audio_groups):\n",
    "\n",
    "    pairs = []\n",
    "    for key in audio_groups:\n",
    "        audio_group = audio_groups[key]\n",
    "        n = len(audio_group)\n",
    "        if n < 2:\n",
    "            continue\n",
    "        for i in range(n-1):\n",
    "            a = audio_group[i]\n",
    "            for j in range(i+1, n):\n",
    "                b = audio_group[j]\n",
    "                t1, t2 = map(lambda idx: dataset[idx].numel(), [a, b])\n",
    "                dt = abs(t1 - t2) / min(t1, t2)\n",
    "                similar_tempos = dt < float('0.01')\n",
    "                having_different_speakers = metadata[a]['speaker'] != metadata[b]['speaker']\n",
    "                if similar_tempos and having_different_speakers:\n",
    "                    pairs.append([a, b])\n",
    "    \n",
    "    print(f'''\n",
    "    Different-Speaker, Similar-Tempo, and Same-Accented-Text Recording Pairs:\n",
    "    \n",
    "    Total Number of Pairs [Datapoints]: {len(pairs)}\n",
    "    ''')\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# ----\n",
    "\n",
    "def display_pair(dataset, i, j):\n",
    "    audios = map(lambda idx: Audio(dataset[idx], rate=SAMPLERATE), [i, j])\n",
    "    for audio in audios:\n",
    "        display(audio)\n",
    "\n",
    "def choose_random_pair(pairs):\n",
    "    return pairs[randint(0, len(pairs))]\n",
    "\n",
    "def display_random_pair(dataset, pairs):\n",
    "    pair = choose_random_pair(pairs)\n",
    "    print(f'\\tPair: {pair}\\n')\n",
    "    display_pair(dataset, *pair)\n",
    "    return pair\n",
    "\n",
    "# ----\n",
    "\n",
    "class MFCCTransformer(torchaudio.transforms.MFCC):\n",
    "\n",
    "    def __init__(self, samplerate, n_mfcc, winlen, hoplen, alpha=None, L=None, N=None, dd=False, rasta=True, postprocess=True):\n",
    "        super().__init__(\n",
    "            sample_rate = samplerate,\n",
    "            n_mfcc = n_mfcc,\n",
    "            melkwargs = {\n",
    "                'n_fft': winlen,\n",
    "                'hop_length': hoplen,\n",
    "                'n_mels': n_mfcc,\n",
    "                'center': True,\n",
    "                'power': 2.0,\n",
    "            },\n",
    "        )\n",
    "        self.alpha       = alpha\n",
    "        self.L           = L\n",
    "        self.N           = N\n",
    "        self.dd          = dd\n",
    "        self.rasta       = rasta\n",
    "        self.postprocess = postprocess\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.alpha:\n",
    "            X = self.pre_emphasis(X, self.alpha)\n",
    "        MFCC_bt = super().forward(X)\n",
    "        if self.L:\n",
    "            MFCC_bt = self.spectral_lifter(MFCC_bt, self.L)\n",
    "        if self.rasta:\n",
    "            MFCC_bt = self.temporal_rasta_lifter(MFCC_bt)\n",
    "        if self.N:\n",
    "            dMFCC_bt = self.delta(MFCC_bt, self.N)\n",
    "            MFCC_bt  = torch.concat([MFCC_bt, dMFCC_bt], 1)\n",
    "            if self.dd:\n",
    "                ddMFCC_bt = self.delta(dMFCC_bt, self.N)\n",
    "                MFCC_bt   = torch.concat([MFCC_bt, ddMFCC_bt], 1)\n",
    "        if self.postprocess:\n",
    "            MFCC_bt = self.normalize(MFCC_bt, bipolar=False)\n",
    "            eps     = torch.finfo(MFCC_bt.dtype).eps\n",
    "            MFCC_bt = torch.log10(MFCC_bt.clamp(eps))\n",
    "            MFCC_bt = mfcc_transformer.rectify_outliers(MFCC_bt)\n",
    "        return MFCC_bt\n",
    "    \n",
    "    @staticmethod\n",
    "    def pre_emphasis(X, alpha):\n",
    "        return X[:,1:] - alpha * X[:,:-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_lifter(MFCC_bt, L):\n",
    "        mel_indices = torch.arange(MFCC_bt.shape[1]).float()\n",
    "        lifter_win  = 1 + (L / 2.0) * torch.sin(torch.pi * mel_indices / L)\n",
    "        lifter_win  = lifter_win.view(1,-1,1)\n",
    "        return lifter_win * MFCC_bt\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(MFCC_bt, N=2):\n",
    "        kernel  = torch.arange(-N, N+1, dtype=torch.float32)\n",
    "        kernel /= torch.square(kernel).sum()\n",
    "        kernel  = kernel.view(1,1,-1).expand(MFCC_bt.shape[1],-1,-1)\n",
    "        return F.conv1d(MFCC_bt, kernel, padding=N, groups=MFCC_bt.shape[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def temporal_rasta_lifter(MFCC_bt):\n",
    "        b = torch.tensor([0.2,0.1,0.0,-0.1,-0.2])\n",
    "        a = torch.tensor([1.0,-0.94])\n",
    "        return __class__.iir(b, a, MFCC_bt)\n",
    "    \n",
    "    @staticmethod\n",
    "    def iir(b, a, X):\n",
    "        assert a.numel() > 0 and a[0] == 1.0, 'a[0] MUST ALWAYS be 1.0'\n",
    "        b, a = __class__.pad_iir_coefs(b,a)\n",
    "        n_taps = a.numel()\n",
    "        Y = torch.zeros_like(X)\n",
    "        for idx in range(n_taps-1, 0, -1):\n",
    "            in_start = n_taps - 1 - idx\n",
    "            in_end   = -idx\n",
    "            out_start = in_start + 1\n",
    "            out_end   = in_end + 1 if in_end < -1 else None\n",
    "            Y[:,:,out_start:out_end] += b[:,:,idx] * X[:,:,in_start:in_end] - a[:,:,idx] * Y[:,:,in_start:in_end]\n",
    "        Y += b[:,:,0] * F.pad(X[:,:,n_taps-1:], (n_taps-1, 0))\n",
    "        return Y\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_iir_coefs(b, a):\n",
    "        n_taps = torch.tensor([b.numel(), a.numel()])\n",
    "        n_pads = n_taps.max() - n_taps\n",
    "        b, a = torch.stack([\n",
    "            F.pad(b, (0, n_pads[0])),\n",
    "            F.pad(a, (0, n_pads[1])),\n",
    "        ]).reshape(2,1,1,-1)\n",
    "        return b, a\n",
    "\n",
    "    @staticmethod\n",
    "    def rectify_outliers(t):\n",
    "        t_ = t.detach().clone()\n",
    "        lo, hi = __class__.compute_whiskers(t_)\n",
    "        t_[t_ < lo], t_[t_ > hi] = lo, hi\n",
    "        return t_\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_whiskers(t):\n",
    "        q1, q3 = t.quantile(0.25), t.quantile(0.75)\n",
    "        iqr    = q3 - q1\n",
    "        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        return lo, hi\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(X, bipolar=False):\n",
    "        min_values = X.min(-1).values.min(-1).values.view(-1,1,1)\n",
    "        max_values = X.max(-1).values.max(-1).values.view(-1,1,1)\n",
    "        normalized = (X - min_values) / (max_values - min_values)\n",
    "        if bipolar:\n",
    "            normalized = 2 * (normalized - 0.5)\n",
    "        return normalized\n",
    "\n",
    "# ----\n",
    "\n",
    "def plot(images, cmap='viridis'):\n",
    "    if images is None:\n",
    "        print('Nothing to show.')\n",
    "        return\n",
    "    n = len(images)\n",
    "    if n == 0:\n",
    "        return\n",
    "    fig, axs = plt.subplots(n, 1, figsize=(10,5))\n",
    "    for idx in range(n):\n",
    "        cax = axs[idx].imshow(images[idx], aspect='auto', origin='lower', cmap=cmap)\n",
    "        fig.colorbar(cax, ax=axs[idx])\n",
    "    plt.show()\n",
    "\n",
    "# ----\n",
    "\n",
    "class CompositeScaler(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ab_subsampling_map = self.generate_ab_subsampling_map()\n",
    "    \n",
    "    def forward(self, X_ft, scale, a=True):\n",
    "        scale = round(scale, 2)\n",
    "        if scale < 0.01 or 0.99 < scale:\n",
    "            raise ValueError(f'`scale` MUST ALWAYS be a positive value whose range is [0.01, 0.99].')\n",
    "        if scale in self.ab_subsampling_map.index:\n",
    "            if not a:\n",
    "                X_ft = torch.flipud(X_ft)\n",
    "            scaled_X_ft = self.subsample_ab(X_ft, *self.ab_subsampling_map.loc[scale])\n",
    "            if not a:\n",
    "                X_ft = torch.flipud(X_ft)\n",
    "                scaled_X_ft = torch.flipud(scaled_X_ft)\n",
    "            if scaled_X_ft.shape[-1] > 0.10 * X_ft.shape[-1]:\n",
    "                return scaled_X_ft\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_ab_subsampling_map():\n",
    "    \n",
    "        records = []\n",
    "        for stride in range(2, 101):\n",
    "            records.append(__class__.generate_ab_subsampling_record(stride, True))\n",
    "            records.append(__class__.generate_ab_subsampling_record(stride, False))\n",
    "            \n",
    "        df = pd.DataFrame.from_records(records)\n",
    "        df = df.sort_values('deviation').groupby(['approx_ratio', 'complement_value']).head(1)\n",
    "        df = df.drop_duplicates('approx_ratio').sort_values('approx_ratio').reset_index(drop=True)\n",
    "        df.drop(['approx_ratio', 'deviation'], axis=1, inplace=True)\n",
    "        \n",
    "        records = df.to_dict(orient='records')\n",
    "        pair_records = [\n",
    "            [x for record in record_pair for x in record.values()] \\\n",
    "            for record_pair in combinations(records, 2)\n",
    "        ]\n",
    "        records = [list(record.values()) + [1, True, 1.0] for record in records]\n",
    "        pair_records = records + pair_records\n",
    "        \n",
    "        columns = [f'{col}_{x}' for x in 'ab' for col in df.columns]\n",
    "        df = pd.DataFrame(pair_records, columns=columns)\n",
    "        \n",
    "        df['ratio'] = df['ratio_a'] / df['ratio_b']\n",
    "        \n",
    "        df['approx_ratio'] = df['ratio'].round(2)\n",
    "        df = df.sort_values(['ratio_a', 'ratio_b'], ascending=False).groupby('approx_ratio').head(1)\n",
    "        df = df.drop_duplicates('approx_ratio').sort_values('approx_ratio')\n",
    "        df.drop(['ratio'], axis=1, inplace=True)\n",
    "        mask = (df['approx_ratio'] < 0.005) | (df['approx_ratio'] > 0.995)\n",
    "        df = df[~mask].reset_index(drop=True)\n",
    "        \n",
    "        df['scale_factor_a'] = df['approx_ratio'].copy()\n",
    "        gt_mask_a = df['ratio_a'] > df['ratio_b']\n",
    "        df.loc[gt_mask_a, 'scale_factor_a'] = round(1 / df.loc[gt_mask_a, 'approx_ratio'], 2)\n",
    "        \n",
    "        df['scale_factor_b'] = df['approx_ratio'].copy()\n",
    "        gt_mask_b = df['ratio_b'] > df['ratio_a']\n",
    "        df.loc[gt_mask_b, 'scale_factor_b'] = round(1 / df.loc[gt_mask_b, 'approx_ratio'], 2)\n",
    "        \n",
    "        df.drop(['ratio_a', 'ratio_b', 'approx_ratio'], axis=1, inplace=True)\n",
    "        \n",
    "        assert all(df['scale_factor_a'] < df['scale_factor_b']), 'Scale A should be normalized to always result in shrinking A [0.01 to 0.99] (i.e. Stretching B)'\n",
    "        \n",
    "        df.drop('scale_factor_b', axis=1, inplace=True)\n",
    "        df.set_index('scale_factor_a', inplace=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_ab_subsampling_record(stride:int, complement_value:bool):\n",
    "        ratio = 1 / stride\n",
    "        if not complement_value:\n",
    "            ratio = 1 - ratio\n",
    "        approx_ratio = round(ratio, 2)\n",
    "        return {\n",
    "            'stride': stride,\n",
    "            'complement_value': complement_value,\n",
    "            'ratio': ratio,\n",
    "            'approx_ratio': approx_ratio,\n",
    "            'deviation': abs(ratio - approx_ratio),\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def subsample_ab(X_ft, stride_a, complement_value_a, stride_b, complement_value_b):\n",
    "        if X_ft.shape[0] != 2 or X_ft.ndim != 3:\n",
    "            raise ValueError(f'`X_ft` MUST ALWAYS be a 3D tensor with its first dimension being of size 2.')\n",
    "\n",
    "        f_bins, t_bins = X_ft.shape[1:]\n",
    "        \n",
    "        masks = torch.stack([\n",
    "            __class__.make_subsampling_mask(f_bins, t_bins, complement_value_a, stride_a), \n",
    "            __class__.make_subsampling_mask(f_bins, t_bins, complement_value_b, stride_b),\n",
    "        ])\n",
    "        \n",
    "        t_lengths = masks[:,0,:].sum(-1)\n",
    "        t_pads    = torch.max(t_lengths) - t_lengths\n",
    "        \n",
    "        scaled_X_ft = torch.stack([\n",
    "            F.pad(\n",
    "                X_ft[idx, masks[idx]].reshape([f_bins, t_lengths[idx]]),\n",
    "                pad   = (0, t_pads[idx]),\n",
    "                value = X_ft[idx].min()\n",
    "            ) for idx in range(2)\n",
    "        ])\n",
    "    \n",
    "        return scaled_X_ft\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_subsampling_mask(f_bins, t_bins, complement_value, stride):\n",
    "        blank_tensor = torch.zeros if complement_value else torch.ones\n",
    "        mask = blank_tensor(f_bins, t_bins, dtype=bool)\n",
    "        mask[:, ::stride] = bool(complement_value)\n",
    "        return mask\n",
    "    \n",
    "    def transformed_indices(self, i, j, scale, a):\n",
    "        return self.indices(True, i, j, scale, a)\n",
    "    \n",
    "    def original_indices(self, i, j, scale, a):\n",
    "        return self.indices(False, i, j, scale, a)\n",
    "    \n",
    "    def indices(self, fwd, i, j, scale, a):\n",
    "        record = self.ab_subsampling_record(scale, a)\n",
    "        if fwd:\n",
    "            return self.transformed_index(i, *record[:2]), self.transformed_index(j, *record[2:])\n",
    "        return self.original_index(i, *record[:2]), self.original_index(j, *record[2:])\n",
    "    \n",
    "    def ab_subsampling_record(self, scale, a):\n",
    "        scale = round(scale, 2)\n",
    "        record = self.ab_subsampling_map.loc[scale].tolist()\n",
    "        if not a:\n",
    "            record[:2], record[2:] = record[2:], record[:2]\n",
    "        return record\n",
    "    \n",
    "    @staticmethod\n",
    "    def original_index(idx, stride, complement_value):\n",
    "        if complement_value:\n",
    "            if stride == 1:\n",
    "                return idx\n",
    "            return (idx + 1) * stride - 1\n",
    "        return idx // (stride - 1) * stride + idx % (stride - 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformed_index(idx, stride, complement_value):\n",
    "        if complement_value:\n",
    "            if stride == 1:\n",
    "                return idx\n",
    "            return (idx + 1) // stride - 1\n",
    "        return idx // stride * (stride - 1) + idx % stride\n",
    "\n",
    "# ----\n",
    "\n",
    "def pair_segmentation(MFCC_bt, t_kernel, t_stride, flattened=True):\n",
    "    if MFCC_bt.shape[0] != 2 or MFCC_bt.ndim != 3:\n",
    "        raise ValueError('`MFCC_bt` MUST be a pair of 2D tensors.')\n",
    "    f_bins, t_bins = MFCC_bt.shape[1:]\n",
    "    AB = F.unfold(\n",
    "        MFCC_bt.unsqueeze(1), \n",
    "        kernel_size = (f_bins, t_kernel), \n",
    "        stride = (f_bins, t_stride),\n",
    "    ).transpose(1,2)\n",
    "    if not flattened:\n",
    "        AB = AB.view(2, -1, f_bins, t_kernel)\n",
    "    return tuple(AB)\n",
    "\n",
    "def cosine_similarity(MFCC_bt, t_kernel, t_stride):\n",
    "    A, B = pair_segmentation(MFCC_bt, t_kernel, t_stride)\n",
    "    S = A @ B.T / (torch.linalg.norm(A) * torch.linalg.norm(B))\n",
    "    return S\n",
    "\n",
    "# ----\n",
    "\n",
    "def similarity_records(scale, a, t_kernel, similarity_matrix, original_t_bins):\n",
    "    \n",
    "    scaler = CompositeScaler()\n",
    "    \n",
    "    t_bins = similarity_matrix.shape[0]\n",
    "    similarities = torch.stack([\n",
    "        torch.arange(t_bins).unsqueeze(-1).expand(-1, t_bins),\n",
    "        torch.arange(t_bins).unsqueeze(0).expand(t_bins, -1),\n",
    "        similarity_matrix,\n",
    "    ])\n",
    "    \n",
    "    original_is, original_js = scaler.original_indices(*similarities[:2], scale, a)\n",
    "    mask = (original_is < original_t_bins) & \\\n",
    "           (original_js < original_t_bins)\n",
    "    \n",
    "    similarities = similarities[:, mask]\n",
    "    n = mask.sum()\n",
    "    \n",
    "    similarities = torch.concat([\n",
    "        torch.stack([\n",
    "            torch.tensor([scale] * n),\n",
    "            torch.tensor([a] * n),\n",
    "            torch.tensor([t_kernel] * n),\n",
    "            torch.abs(similarities[0] - similarities[1]),\n",
    "        ]),\n",
    "        similarities,\n",
    "    ]).T\n",
    "    \n",
    "    similarities = pd.DataFrame(similarities, columns=['scale', 'a', 't_kernel', 'delta', 'i', 'j', 'similarity'])\n",
    "    \n",
    "    int_columns = similarities.columns[1:-1]\n",
    "    dtype_dict = dict(zip(int_columns, [int] * len(int_columns)))\n",
    "    similarities = similarities.astype(dtype_dict)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def assert_dirpath(filepath):\n",
    "    filepath = os.path.abspath(filepath)\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "def export_records(all_records, export_filepath):\n",
    "    assert_dirpath(export_filepath)\n",
    "    all_records = all_records.sort_values(by=['similarity', 'delta', 'scale'], ascending=[False, True, False])\n",
    "    all_records.to_csv(export_filepath, index=False)\n",
    "\n",
    "def matching_frames(MFCC_bt, scale_range, t_kernel_range, t_stride, export_dirpath='./matching-records', chunk_size=None):\n",
    "    \n",
    "    scaler, all_records = CompositeScaler(), pd.DataFrame()\n",
    "    original_t_bins = MFCC_bt.shape[-1]\n",
    "    iter_, n_iters = 0, len(t_kernel_range) * len(scale_range) * 2\n",
    "    exports = 0\n",
    "    \n",
    "    for t_kernel in t_kernel_range:\n",
    "        for scale in scale_range:\n",
    "            for a in range(2):\n",
    "\n",
    "                print(f'\\rDynamic Time Warping - Resizing/Matching Frames [{(iter_ + 1)/ n_iters:.2%}] - Window Size: {t_kernel}, Scale: {scale:.0%}, Scaling: {\"A\" if a else \"B\"}', end='')\n",
    "                \n",
    "                scaled_MFCC_bt = scaler(MFCC_bt, scale, a)\n",
    "                if (scaled_MFCC_bt is None) or (t_kernel >= scaled_MFCC_bt.shape[-1]):\n",
    "                    continue\n",
    "                \n",
    "                similarity_matrix = cosine_similarity(scaled_MFCC_bt, t_kernel, t_stride)\n",
    "                records = similarity_records(\n",
    "                    scale, a, t_kernel,\n",
    "                    similarity_matrix,\n",
    "                    original_t_bins,\n",
    "                )\n",
    "                \n",
    "                all_records = pd.concat([all_records, records])\n",
    "\n",
    "                if chunk_size and len(all_records) > chunk_size:\n",
    "                    exports += 1\n",
    "                    export_filepath = os.path.join(export_dirpath, f'{exports}.csv')\n",
    "                    export_records(all_records, export_filepath)\n",
    "                    all_records.drop(all_records.index, inplace=True)\n",
    "                \n",
    "                iter_ += 1\n",
    "\n",
    "    if chunk_size:\n",
    "        exports += 1\n",
    "        export_filepath = os.path.join(export_dirpath, f'{exports}.csv')\n",
    "        export_records(all_records, export_filepath)\n",
    "        all_records.drop(all_records.index, inplace=True)\n",
    "        return\n",
    "\n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e310f2-5b16-49c0-aa5f-25d06e0516d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eefcc28e27441c7a7a1f464743b8493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmu = load_dataset('CLAPv2/CMU_Arctic')\n",
    "cmu = concatenate_datasets([*cmu.values()])\n",
    "\n",
    "metadata, dataset, samplerates = zip(*map(get_audio_attrs, cmu))\n",
    "\n",
    "# ----\n",
    "\n",
    "def compute_PdB_t(A_ft):\n",
    "    assert 1 < A_ft.ndim < 4, '`A_ft` MUST be 2D in case it does not have a batch dimension.'\n",
    "    f_axis = 0 if A_ft.ndim < 3 else 1\n",
    "    return 10 * torch.log10(torch.square(A_ft).sum(f_axis) + 1e-6)\n",
    "\n",
    "def apply_vad(Spectrogram, InvSpectrogram, samples, dB_thd=-3):\n",
    "    X_ft         = Spectrogram(samples)\n",
    "    A_ft, phi_ft = map(lambda func: func(X_ft), [torch.abs, torch.angle])\n",
    "    PdB_t        = compute_PdB_t(A_ft)\n",
    "    f_bins       = A_ft.shape[0]\n",
    "    vad_ft       = (PdB_t >= dB_thd).unsqueeze(0).expand(f_bins, -1)\n",
    "    A_ft, phi_ft = map(lambda M_ft: M_ft[vad_ft].reshape(f_bins, -1), [A_ft, phi_ft])\n",
    "    X_ft         = A_ft * torch.exp(1j * phi_ft)\n",
    "    samples      = InvSpectrogram(X_ft)\n",
    "    return samples\n",
    "\n",
    "ms_to_samples = lambda time_ms, samplerate: time_ms * samplerate // 1000\n",
    "\n",
    "# ----\n",
    "\n",
    "unique_samplerates = list(set(samplerates))\n",
    "assert len(unique_samplerates) == 1\n",
    "SAMPLERATE:final = unique_samplerates[0]\n",
    "ms_to_samples = partial(ms_to_samples, samplerate=SAMPLERATE)\n",
    "\n",
    "WIN_MS, HOP_MS   = 15, 5\n",
    "WIN_LEN, HOP_LEN = map(ms_to_samples, [WIN_MS, HOP_MS])\n",
    "\n",
    "Spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    WIN_LEN, WIN_LEN, HOP_LEN, \n",
    "    0, torch.hann_window, None,\n",
    ")\n",
    "\n",
    "InvSpectrogram = torchaudio.transforms.InverseSpectrogram(\n",
    "    WIN_LEN, WIN_LEN, HOP_LEN, \n",
    "    0, torch.hann_window,\n",
    ")\n",
    "\n",
    "apply_vad    = partial(apply_vad, Spectrogram, InvSpectrogram)\n",
    "# dataset      = tuple(map(apply_vad, dataset))\n",
    "audio_groups = get_audio_groups(metadata, dataset)\n",
    "audio_pairs  = get_matching_pairs(metadata, dataset, audio_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eb910-24e6-4dcd-a0fc-6275e777d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i, j = display_random_pair(dataset, audio_pairs)\n",
    "i, j = [3595, 8352]; display_pair(dataset, i, j)\n",
    "\n",
    "X_t = pad_sequence([dataset[i], dataset[j]], padding_value=dataset[i].min()).T\n",
    "\n",
    "mfcc_transformer = MFCCTransformer(samplerate  = SAMPLERATE,\n",
    "                                   n_mfcc      = 2,\n",
    "                                   winlen      = WIN_LEN,\n",
    "                                   hoplen      = HOP_LEN,\n",
    "                                   alpha       = 0.0,\n",
    "                                   L           = 0,\n",
    "                                   N           = 0,\n",
    "                                   dd          = False,\n",
    "                                   rasta       = False,\n",
    "                                   postprocess = False)\n",
    "\n",
    "MFCC_bt = mfcc_transformer(X_t)\n",
    "plot(MFCC_bt, 'viridis')\n",
    "\n",
    "scaler = CompositeScaler()\n",
    "\n",
    "SCALE_RANGE    = np.arange(0.20, 1.00, 0.01)\n",
    "T_KERNEL_RANGE = np.arange(14, 100, 1)\n",
    "T_STRIDE       = 1\n",
    "EXPORT_DIRPATH = './matching-frames'\n",
    "CHUNK_SIZE     = 20_000_000\n",
    "\n",
    "matching_records = matching_frames(MFCC_bt, SCALE_RANGE, T_KERNEL_RANGE, T_STRIDE, EXPORT_DIRPATH, CHUNK_SIZE)\n",
    "\n",
    "best_record = matching_records.head(1).to_dict('records')[0]\n",
    "scale, a, t_kernel, _, i, j, _ = best_record.values()\n",
    "\n",
    "scaled_MFCC_bt = scaler(MFCC_bt, scale, a)\n",
    "A, B = pair_segmentation(MFCC_bt, t_kernel, t_stride=1, flattened=False)\n",
    "\n",
    "best_match = torch.stack([A[i], B[j]])\n",
    "\n",
    "print(best_record)\n",
    "print('-' * 38 + ' Best Match ' + '-' * 38)\n",
    "plot(best_match, 'viridis')\n",
    "\n",
    "original_i0, original_j0 = scaler.original_indices(i, j, scale, a)\n",
    "original_i1, original_j1 = scaler.original_indices(i + t_kernel, j + t_kernel, scale, a)\n",
    "island_a, island_b = slice(original_i0, original_i1), slice(original_j0, original_j1)\n",
    "print(island_a, island_b)\n",
    "\n",
    "X_ft = Spectrogram(X_t)\n",
    "\n",
    "samples_a = InvSpectrogram(X_ft[0, :, island_a].unsqueeze(0))\n",
    "audio_a = Audio(samples_a, rate=SAMPLERATE)\n",
    "display(audio_a)\n",
    "\n",
    "samples_b = InvSpectrogram(X_ft[1, :, island_b].unsqueeze(0))\n",
    "audio_b = Audio(samples_b, rate=SAMPLERATE)\n",
    "display(audio_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
