{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d964c8-18d1-4181-b1c7-f407d142e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c43b9e72a834d2793e562beee78f743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'datasetname', 'audio', 'audio_len', 'text', 'raw_text'],\n",
       "    num_rows: 13192\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': './train/10005',\n",
       " 'datasetname': 'FSD50K',\n",
       " 'audio': <datasets.features._torchcodec.AudioDecoder at 0x7fc335b638c0>,\n",
       " 'audio_len': 3.17006254196167,\n",
       " 'text': 'A man reads out \"He was fond of quoting a fragment from a certain poem.\" in the Scottish accent',\n",
       " 'raw_text': ['Title: CMU_Arctic',\n",
       "  'Description: The databases consist of around 1150 utterances carefully selected from out-of-copyright texts from Project Gutenberg. The databses include US English male (bdl) and female (slt) speakers (both experinced voice talent) as well as other accented speakers.',\n",
       "  'License: BSD',\n",
       "  'Text: He was fond of quoting a fragment from a certain poem.',\n",
       "  'Accent: Scottish',\n",
       "  'Gender: male',\n",
       "  'Filename: cmu_sc_male/arctic_a0520.wav']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "cmu = load_dataset('CLAPv2/CMU_Arctic')\n",
    "cmu = concatenate_datasets([*cmu.values()])\n",
    "display(cmu)\n",
    "print(f\"\\n{'-' * 50}\\n\")\n",
    "display(cmu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7677804a-ea8e-4eac-8bcd-1b80d001e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_metadata(record):\n",
    "    info     = '\\n'.join(record['raw_text'])\n",
    "    patterns = { \n",
    "        'speaker'    : r'Filename:\\s*([^/]+)',\n",
    "        'accent'     : r'Accent:\\s*([^\\n]+)', \n",
    "        'sex'        : r'Gender:\\s*([^\\n]+)',\n",
    "        'spoken_text': r'Text:\\s*([^\\n]+)',\n",
    "    }\n",
    "    extract_pattern = lambda pattern: re.search(pattern, info).group(1).strip()\n",
    "    metadata = { \n",
    "        key: extract_pattern(patterns[key]) for key in patterns\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "def get_audio(record):\n",
    "    audio = record['audio'].get_all_samples()\n",
    "    samples = audio.data.mean(0)\n",
    "    samplerate = audio.sample_rate\n",
    "    return samples, samplerate\n",
    "\n",
    "def get_audio_attrs(record):\n",
    "    metadata            = get_metadata(record)\n",
    "    dataset, samplerate = get_audio(record)\n",
    "    return metadata, dataset, samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921a0fe-2467-4de7-82ad-0e88ae034109",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, dataset, samplerates = zip(*map(get_audio_attrs, cmu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c137f4-c548-4fe4-9478-6047514d8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import final\n",
    "\n",
    "unique_samplerates = list(set(samplerates))\n",
    "assert len(unique_samplerates) == 1, \"When executing this check all the audios should've been already subsampled/supersampled to the same sampling rate.\"\n",
    "\n",
    "SAMPLERATE: final = unique_samplerates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212848a-b76e-4257-821f-76f1159b61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "WIN_MS, HOP_MS   = 15, 5\n",
    "WIN_LEN, HOP_LEN = map(lambda time_ms: time_ms * SAMPLERATE // 1000, [WIN_MS, HOP_MS])\n",
    "\n",
    "Spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    WIN_LEN, WIN_LEN, HOP_LEN, \n",
    "    0, torch.hann_window, None,\n",
    ")\n",
    "\n",
    "InvSpectrogram = torchaudio.transforms.InverseSpectrogram(\n",
    "    WIN_LEN, WIN_LEN, HOP_LEN, \n",
    "    0, torch.hann_window,\n",
    ")\n",
    "\n",
    "def compute_PdB_t(A_ft):\n",
    "    assert 1 < A_ft.ndim < 4, '`A_ft` MUST be 2D in case it does not have a batch dimension.'\n",
    "    f_axis = 0 if A_ft.ndim < 3 else 1\n",
    "    return 10 * torch.log10(torch.square(A_ft).sum(f_axis) + 1e-6)\n",
    "\n",
    "def apply_vad(samples, dB_thd=-3):\n",
    "    global Spectrogram, InvSpectrogram\n",
    "    X_ft         = Spectrogram(samples)\n",
    "    A_ft, phi_ft = map(lambda func: func(X_ft), [torch.abs, torch.angle])\n",
    "    PdB_t        = compute_PdB_t(A_ft)\n",
    "    f_bins       = A_ft.shape[0]\n",
    "    vad_ft       = (PdB_t >= dB_thd).unsqueeze(0).expand(f_bins, -1)\n",
    "    A_ft, phi_ft = map(lambda M_ft: M_ft[vad_ft].reshape(f_bins, -1), [A_ft, phi_ft])\n",
    "    X_ft         = A_ft * torch.exp(1j * phi_ft)\n",
    "    samples      = InvSpectrogram(X_ft)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccdd2a-4627-4f3f-b0e8-a8467fe1a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tuple(map(apply_vad, dataset))\n",
    "MAX_SAMPLES: final = max(map(len, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c32db7-16ca-4fb1-b337-1f173355acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_audio_groups(metadata, dataset):\n",
    "    \n",
    "    key_components = ['accent', 'spoken_text']\n",
    "    \n",
    "    audio_groups = defaultdict(list)\n",
    "    for idx, audio in enumerate(metadata):\n",
    "        key = tuple(audio[x] for x in key_components)\n",
    "        audio_groups[key].append(idx)\n",
    "    \n",
    "    accents, spoken_texts = map(set, zip(*audio_groups.keys()))\n",
    "    \n",
    "    print(f'''\n",
    "    Original Recordings:\n",
    "    \n",
    "    Total Number of Recordings:      {len(cmu)}\n",
    "    Number of Accents:               {len(accents)}\n",
    "    Number of Spoken Texts:          {len(spoken_texts)}\n",
    "    Number of Accented Spoken Texts: {len(audio_groups)}\n",
    "    ''')\n",
    "\n",
    "    return audio_groups\n",
    "\n",
    "audio_groups = get_audio_groups(metadata, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849e395-4c9d-4064-83a1-364c86562fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_pairs(metadata, dataset, audio_groups):\n",
    "\n",
    "    pairs = []\n",
    "    for key in audio_groups:\n",
    "        audio_group = audio_groups[key]\n",
    "        n = len(audio_group)\n",
    "        if n < 2:\n",
    "            continue\n",
    "        for i in range(n-1):\n",
    "            a = audio_group[i]\n",
    "            for j in range(i+1, n):\n",
    "                b = audio_group[j]\n",
    "                t1, t2 = map(lambda idx: dataset[idx].numel(), [a, b])\n",
    "                dt = abs(t1 - t2) / min(t1, t2)\n",
    "                similar_tempos = dt < float('0.01')\n",
    "                having_different_speakers = metadata[a]['speaker'] != metadata[b]['speaker']\n",
    "                if similar_tempos and having_different_speakers:\n",
    "                    pairs.append([a, b])\n",
    "    \n",
    "    print(f'''\n",
    "    Different-Speaker, Similar-Tempo, and Same-Accented-Text Recording Pairs:\n",
    "    \n",
    "    Total Number of Pairs [Datapoints]: {len(pairs)}\n",
    "    ''')\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "pairs = get_matching_pairs(metadata, dataset, audio_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70559d-734d-498d-859c-efdedfb5ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "from random import randint\n",
    "\n",
    "def display_pair(dataset, i, j):\n",
    "    audios = map(lambda idx: Audio(dataset[idx], rate=SAMPLERATE), [i, j])\n",
    "    for audio in audios:\n",
    "        display(audio)\n",
    "\n",
    "def choose_random_pair(pairs):\n",
    "    return pairs[randint(0, len(pairs))]\n",
    "\n",
    "def display_random_pair(dataset, pairs):\n",
    "    pair = choose_random_pair(pairs)\n",
    "    print(f'\\tPair: {pair}\\n')\n",
    "    display_pair(dataset, *pair)\n",
    "    return pair\n",
    "\n",
    "i, j = choose_random_pair(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff6226-995b-499e-a8a4-ba6cf33026d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "\n",
    "class MFCCTransformer(torchaudio.transforms.MFCC):\n",
    "\n",
    "    def __init__(self, samplerate, n_mfcc, winlen, hoplen, alpha=None, L=None, N=None, dd=False, rasta=True, postprocess=True):\n",
    "        super().__init__(\n",
    "            sample_rate = samplerate,\n",
    "            n_mfcc = n_mfcc,\n",
    "            melkwargs = {\n",
    "                'n_fft': winlen,\n",
    "                'hop_length': hoplen,\n",
    "                'n_mels': n_mfcc,\n",
    "                'center': True,\n",
    "                'power': 2.0,\n",
    "            },\n",
    "        )\n",
    "        self.alpha       = alpha\n",
    "        self.L           = L\n",
    "        self.N           = N\n",
    "        self.dd          = dd\n",
    "        self.rasta       = rasta\n",
    "        self.postprocess = postprocess\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.alpha:\n",
    "            X = self.pre_emphasis(X, self.alpha)\n",
    "        MFCC_bt = super().forward(X)\n",
    "        if self.L:\n",
    "            MFCC_bt = self.spectral_lifter(MFCC_bt, self.L)\n",
    "        if self.rasta:\n",
    "            MFCC_bt = self.temporal_rasta_lifter(MFCC_bt)\n",
    "        if self.N:\n",
    "            dMFCC_bt = self.delta(MFCC_bt, self.N)\n",
    "            MFCC_bt  = torch.concat([MFCC_bt, dMFCC_bt], 1)\n",
    "            if self.dd:\n",
    "                ddMFCC_bt = self.delta(dMFCC_bt, self.N)\n",
    "                MFCC_bt   = torch.concat([MFCC_bt, ddMFCC_bt], 1)\n",
    "        if self.postprocess:\n",
    "            MFCC_bt = self.normalize(MFCC_bt, bipolar=False)\n",
    "            eps     = torch.finfo(MFCC_bt.dtype).eps\n",
    "            MFCC_bt = torch.log10(MFCC_bt.clamp(eps))\n",
    "            MFCC_bt = mfcc_transformer.rectify_outliers(MFCC_bt)\n",
    "        return MFCC_bt\n",
    "    \n",
    "    @staticmethod\n",
    "    def pre_emphasis(X, alpha):\n",
    "        return X[:,1:] - alpha * X[:,:-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_lifter(MFCC_bt, L):\n",
    "        mel_indices = torch.arange(MFCC_bt.shape[1]).float()\n",
    "        lifter_win  = 1 + (L / 2.0) * torch.sin(torch.pi * mel_indices / L)\n",
    "        lifter_win  = lifter_win.view(1,-1,1)\n",
    "        return lifter_win * MFCC_bt\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(MFCC_bt, N=2):\n",
    "        kernel  = torch.arange(-N, N+1, dtype=torch.float32)\n",
    "        kernel /= torch.square(kernel).sum()\n",
    "        kernel  = kernel.view(1,1,-1).expand(MFCC_bt.shape[1],-1,-1)\n",
    "        return F.conv1d(MFCC_bt, kernel, padding=N, groups=MFCC_bt.shape[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def temporal_rasta_lifter(MFCC_bt):\n",
    "        b = torch.tensor([0.2,0.1,0.0,-0.1,-0.2])\n",
    "        a = torch.tensor([1.0,-0.94])\n",
    "        return __class__.iir(b, a, MFCC_bt)\n",
    "    \n",
    "    @staticmethod\n",
    "    def iir(b, a, X):\n",
    "        assert a.numel() > 0 and a[0] == 1.0, 'a[0] MUST ALWAYS be 1.0'\n",
    "        b, a = __class__.pad_iir_coefs(b,a)\n",
    "        n_taps = a.numel()\n",
    "        Y = torch.zeros_like(X)\n",
    "        for idx in range(n_taps-1, 0, -1):\n",
    "            in_start = n_taps - 1 - idx\n",
    "            in_end   = -idx\n",
    "            out_start = in_start + 1\n",
    "            out_end   = in_end + 1 if in_end < -1 else None\n",
    "            Y[:,:,out_start:out_end] += b[:,:,idx] * X[:,:,in_start:in_end] - a[:,:,idx] * Y[:,:,in_start:in_end]\n",
    "        Y += b[:,:,0] * F.pad(X[:,:,n_taps-1:], (n_taps-1, 0))\n",
    "        return Y\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_iir_coefs(b, a):\n",
    "        n_taps = torch.tensor([b.numel(), a.numel()])\n",
    "        n_pads = n_taps.max() - n_taps\n",
    "        b, a = torch.stack([\n",
    "            F.pad(b, (0, n_pads[0])),\n",
    "            F.pad(a, (0, n_pads[1])),\n",
    "        ]).reshape(2,1,1,-1)\n",
    "        return b, a\n",
    "\n",
    "    @staticmethod\n",
    "    def rectify_outliers(t):\n",
    "        t_ = t.detach().clone()\n",
    "        lo, hi = __class__.compute_whiskers(t_)\n",
    "        t_[t_ < lo], t_[t_ > hi] = lo, hi\n",
    "        return t_\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_whiskers(t):\n",
    "        q1, q3 = t.quantile(0.25), t.quantile(0.75)\n",
    "        iqr    = q3 - q1\n",
    "        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        return lo, hi\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(X, bipolar=False):\n",
    "        min_values = X.min(-1).values.min(-1).values.view(-1,1,1)\n",
    "        max_values = X.max(-1).values.max(-1).values.view(-1,1,1)\n",
    "        normalized = (X - min_values) / (max_values - min_values)\n",
    "        if bipolar:\n",
    "            normalized = 2 * (normalized - 0.5)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecddd77-a5fc-448f-a578-e58580d16093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "X_t = pad_sequence([dataset[i], dataset[j]]).T\n",
    "\n",
    "mfcc_transformer = MFCCTransformer(samplerate  = SAMPLERATE,\n",
    "                                   n_mfcc      = 13,\n",
    "                                   winlen      = WIN_LEN,\n",
    "                                   hoplen      = HOP_LEN,\n",
    "                                   alpha       = 0.97,\n",
    "                                   L           = 6,\n",
    "                                   N           = 2,\n",
    "                                   dd          = True,\n",
    "                                   rasta       = True,\n",
    "                                   postprocess = True)\n",
    "\n",
    "MFCC_bt = mfcc_transformer(X_t)\n",
    "MFCC_bt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707736b-1534-4a4e-9dd8-844ec28846d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(images, cmap='viridis'):\n",
    "    if images is None:\n",
    "        print('Nothing to show.')\n",
    "        return\n",
    "    n = len(images)\n",
    "    if n == 0:\n",
    "        return\n",
    "    fig, axs = plt.subplots(n, 1, figsize=(10,5))\n",
    "    for idx in range(n):\n",
    "        cax = axs[idx].imshow(images[idx], aspect='auto', origin='lower', cmap=cmap)\n",
    "        fig.colorbar(cax, ax=axs[idx])\n",
    "    plt.show()\n",
    "\n",
    "plot(MFCC_bt, 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c27679-921c-4119-a441-652d027ec832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "X_ft = Spectrogram(X_t)\n",
    "\n",
    "A_ft   = torch.abs(X_ft)\n",
    "phi_ft = torch.angle(X_ft)\n",
    "\n",
    "PdB_ft = 10 * torch.log10(A_ft + 1e-6)\n",
    "plot(PdB_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13175d-706c-4bb2-80f2-7e238b581d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "class CompositeScaler(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ab_subsampling_map = self.generate_ab_subsampling_map()\n",
    "    \n",
    "    def forward(self, X_ft, scale, a=True):\n",
    "        scale = round(scale, 2)\n",
    "        if scale < 0.01 or 0.99 < scale:\n",
    "            raise ValueError(f'`scale` MUST ALWAYS be a positive value whose range is [0.01, 0.99].')\n",
    "        if scale in self.ab_subsampling_map.index:\n",
    "            if not a:\n",
    "                X_ft = torch.flipud(X_ft)\n",
    "            scaled_X_ft = self.subsample_ab(X_ft, *self.ab_subsampling_map.loc[scale])\n",
    "            if not a:\n",
    "                X_ft = torch.flipud(X_ft)\n",
    "                scaled_X_ft = torch.flipud(scaled_X_ft)\n",
    "            if scaled_X_ft.shape[-1] > 0.10 * X_ft.shape[-1]:\n",
    "                return scaled_X_ft\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_ab_subsampling_map():\n",
    "    \n",
    "        records = []\n",
    "        for stride in range(2, 101):\n",
    "            records.append(__class__.generate_ab_subsampling_record(stride, True))\n",
    "            records.append(__class__.generate_ab_subsampling_record(stride, False))\n",
    "            \n",
    "        df = pd.DataFrame.from_records(records)\n",
    "        df = df.sort_values('deviation').groupby(['approx_ratio', 'complement_value']).head(1)\n",
    "        df = df.drop_duplicates('approx_ratio').sort_values('approx_ratio').reset_index(drop=True)\n",
    "        df.drop(['approx_ratio', 'deviation'], axis=1, inplace=True)\n",
    "        \n",
    "        records = df.to_dict(orient='records')\n",
    "        pair_records = [\n",
    "            [x for record in record_pair for x in record.values()] \\\n",
    "            for record_pair in combinations(records, 2)\n",
    "        ]\n",
    "        records = [list(record.values()) + [1, True, 1.0] for record in records]\n",
    "        pair_records = records + pair_records\n",
    "        \n",
    "        columns = [f'{col}_{x}' for x in 'ab' for col in df.columns]\n",
    "        df = pd.DataFrame(pair_records, columns=columns)\n",
    "        \n",
    "        df['ratio'] = df['ratio_a'] / df['ratio_b']\n",
    "        \n",
    "        df['approx_ratio'] = df['ratio'].round(2)\n",
    "        df = df.sort_values(['ratio_a', 'ratio_b'], ascending=False).groupby('approx_ratio').head(1)\n",
    "        df = df.drop_duplicates('approx_ratio').sort_values('approx_ratio')\n",
    "        df.drop(['ratio'], axis=1, inplace=True)\n",
    "        mask = (df['approx_ratio'] < 0.005) | (df['approx_ratio'] > 0.995)\n",
    "        df = df[~mask].reset_index(drop=True)\n",
    "        \n",
    "        df['scale_factor_a'] = df['approx_ratio'].copy()\n",
    "        gt_mask_a = df['ratio_a'] > df['ratio_b']\n",
    "        df.loc[gt_mask_a, 'scale_factor_a'] = round(1 / df.loc[gt_mask_a, 'approx_ratio'], 2)\n",
    "        \n",
    "        df['scale_factor_b'] = df['approx_ratio'].copy()\n",
    "        gt_mask_b = df['ratio_b'] > df['ratio_a']\n",
    "        df.loc[gt_mask_b, 'scale_factor_b'] = round(1 / df.loc[gt_mask_b, 'approx_ratio'], 2)\n",
    "        \n",
    "        df.drop(['ratio_a', 'ratio_b', 'approx_ratio'], axis=1, inplace=True)\n",
    "        \n",
    "        assert all(df['scale_factor_a'] < df['scale_factor_b']), 'Scale A should be normalized to always result in shrinking A [0.01 to 0.99] (i.e. Stretching B)'\n",
    "        \n",
    "        df.drop('scale_factor_b', axis=1, inplace=True)\n",
    "        df.set_index('scale_factor_a', inplace=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_ab_subsampling_record(stride:int, complement_value:bool):\n",
    "        ratio = 1 / stride\n",
    "        if not complement_value:\n",
    "            ratio = 1 - ratio\n",
    "        approx_ratio = round(ratio, 2)\n",
    "        return {\n",
    "            'stride': stride,\n",
    "            'complement_value': complement_value,\n",
    "            'ratio': ratio,\n",
    "            'approx_ratio': approx_ratio,\n",
    "            'deviation': abs(ratio - approx_ratio),\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def subsample_ab(X_ft, stride_a, complement_value_a, stride_b, complement_value_b):\n",
    "        if X_ft.shape[0] != 2 or X_ft.ndim != 3:\n",
    "            raise ValueError(f'`X_ft` MUST ALWAYS be a 3D tensor with its first dimension being of size 2.')\n",
    "\n",
    "        f_bins, t_bins = X_ft.shape[1:]\n",
    "        \n",
    "        masks = torch.stack([\n",
    "            __class__.make_subsampling_mask(f_bins, t_bins, complement_value_a, stride_a), \n",
    "            __class__.make_subsampling_mask(f_bins, t_bins, complement_value_b, stride_b),\n",
    "        ])\n",
    "        \n",
    "        t_lengths = masks[:,0,:].sum(-1)\n",
    "        t_pads    = torch.max(t_lengths) - t_lengths\n",
    "        \n",
    "        scaled_X_ft = torch.stack([\n",
    "            F.pad(\n",
    "                X_ft[idx, masks[idx]].reshape([f_bins, t_lengths[idx]]),\n",
    "                pad   = (0, t_pads[idx]),\n",
    "                value = X_ft[idx].min()\n",
    "            ) for idx in range(2)\n",
    "        ])\n",
    "    \n",
    "        return scaled_X_ft\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_subsampling_mask(f_bins, t_bins, complement_value, stride):\n",
    "        blank_tensor = torch.zeros if complement_value else torch.ones\n",
    "        mask = blank_tensor(f_bins, t_bins, dtype=bool)\n",
    "        mask[:, ::stride] = bool(complement_value)\n",
    "        return mask\n",
    "    \n",
    "    def transformed_indices(self, i, j, scale, a):\n",
    "        return self.indices(True, i, j, scale, a)\n",
    "    \n",
    "    def original_indices(self, i, j, scale, a):\n",
    "        return self.indices(False, i, j, scale, a)\n",
    "    \n",
    "    def indices(self, fwd, i, j, scale, a):\n",
    "        record = self.ab_subsampling_record(scale, a)\n",
    "        if fwd:\n",
    "            return self.transformed_index(i, *record[:2]), self.transformed_index(j, *record[2:])\n",
    "        return self.original_index(i, *record[:2]), self.original_index(j, *record[2:])\n",
    "    \n",
    "    def ab_subsampling_record(self, scale, a):\n",
    "        scale = round(scale, 2)\n",
    "        record = self.ab_subsampling_map.loc[scale].tolist()\n",
    "        if not a:\n",
    "            record[:2], record[2:] = record[2:], record[:2]\n",
    "        return record\n",
    "    \n",
    "    @staticmethod\n",
    "    def original_index(idx, stride, complement_value):\n",
    "        if complement_value:\n",
    "            if stride == 1:\n",
    "                return idx\n",
    "            return (idx + 1) * stride - 1\n",
    "        return idx // (stride - 1) * stride + idx % (stride - 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformed_index(idx, stride, complement_value):\n",
    "        if complement_value:\n",
    "            if stride == 1:\n",
    "                return idx\n",
    "            return (idx + 1) // stride - 1\n",
    "        return idx // stride * (stride - 1) + idx % stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48929832-0c9a-4d4f-b533-0b1efe6a5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = CompositeScaler()\n",
    "\n",
    "print('-' * 38 + ' Scaling A ' + '-' * 38)\n",
    "scaled_MFCC_bt = scaler(MFCC_bt, 0.96, a=True)\n",
    "plot(scaled_MFCC_bt)\n",
    "print('\\n')\n",
    "print('-' * 38 + ' Scaling B ' + '-' * 38)\n",
    "scaled_MFCC_bt = scaler(MFCC_bt, 0.96, a=False)\n",
    "plot(scaled_MFCC_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a161358-ad22-470c-b9f6-d404d8ae92c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pair_segmentation(MFCC_bt, t_kernel, t_stride, flattened=True):\n",
    "    if MFCC_bt.shape[0] != 2 or MFCC_bt.ndim != 3:\n",
    "        raise ValueError('`MFCC_bt` MUST be a pair of 2D tensors.')\n",
    "    f_bins, t_bins = MFCC_bt.shape[1:]\n",
    "    AB = F.unfold(\n",
    "        MFCC_bt.unsqueeze(1), \n",
    "        kernel_size = (f_bins, t_kernel), \n",
    "        stride = (f_bins, t_stride),\n",
    "    ).transpose(1,2)\n",
    "    if not flattened:\n",
    "        AB = AB.view(2, -1, f_bins, t_kernel)\n",
    "    return tuple(AB)\n",
    "\n",
    "def cosine_similarity(MFCC_bt, t_kernel, t_stride):\n",
    "    A, B = pair_segmentation(MFCC_bt, t_kernel, t_stride)\n",
    "    S = A @ B.T / (torch.linalg.norm(A) * torch.linalg.norm(B))\n",
    "    return S\n",
    "\n",
    "similarity_matrix = cosine_similarity(MFCC_bt, t_kernel=40, t_stride=1)\n",
    "plt.imshow(similarity_matrix, origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffeccd-0753-4bc2-beee-409e94574222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34b9a8-b644-44ce-8e74-64b45b75d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similarity_records(scale, a, t_kernel, similarity_matrix, original_t_bins):\n",
    "    \n",
    "    scaler = CompositeScaler()\n",
    "    \n",
    "    t_bins = similarity_matrix.shape[0]\n",
    "    similarities = torch.stack([\n",
    "        torch.arange(t_bins).unsqueeze(-1).expand(-1, t_bins),\n",
    "        torch.arange(t_bins).unsqueeze(0).expand(t_bins, -1),\n",
    "        similarity_matrix,\n",
    "    ])\n",
    "    \n",
    "    original_is, original_js = scaler.original_indices(*similarities[:2], 0.40, 0)\n",
    "    mask = (original_is < original_t_bins) & \\\n",
    "           (original_js < original_t_bins)\n",
    "    \n",
    "    similarities = similarities[:, mask]\n",
    "    n = mask.sum()\n",
    "    \n",
    "    similarities = torch.concat([\n",
    "        torch.stack([\n",
    "            torch.tensor([scale] * n),\n",
    "            torch.tensor([a] * n),\n",
    "            torch.tensor([t_kernel] * n),\n",
    "            torch.abs(similarities[0] - similarities[1]),\n",
    "        ]),\n",
    "        similarities,\n",
    "    ]).T\n",
    "    \n",
    "    similarities = pd.DataFrame(similarities, columns=['scale', 'a', 't_kernel', 'delta', 'i', 'j', 'similarity'])\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def matching_frames(MFCC_bt, scale_range, t_kernel=40, t_stride=1):\n",
    "    \n",
    "    scaler, all_records = CompositeScaler(), pd.DataFrame()\n",
    "    original_t_bins = MFCC_bt.shape[-1]\n",
    "    \n",
    "    for scale in scale_range:\n",
    "        for a in range(2):\n",
    "            \n",
    "            scaled_MFCC_bt = scaler(MFCC_bt, scale, a)\n",
    "            if (scaled_MFCC_bt is None) or (t_kernel >= scaled_MFCC_bt.shape[-1]):\n",
    "                continue\n",
    "            \n",
    "            similarity_matrix = cosine_similarity(scaled_MFCC_bt, t_kernel, t_stride)\n",
    "            records = similarity_records(\n",
    "                scale, a, t_kernel,\n",
    "                similarity_matrix,\n",
    "                original_t_bins,\n",
    "            )\n",
    "            \n",
    "            all_records = pd.concat([all_records, records])\n",
    "    \n",
    "    return all_records.sort_values(by=['similarity', 'delta', 'scale'], ascending=[False, True, False])\n",
    "\n",
    "\n",
    "SCALE_RANGE = np.arange(0.10, 1.00, 0.01)\n",
    "T_KERNEL    = 40\n",
    "\n",
    "matching_records = matching_frames(MFCC_bt, SCALE_RANGE, T_KERNEL)\n",
    "matching_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8ba0e-bc2f-4a5d-8f59-cf23de49adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match = matching_records.iloc[0].to_dict()\n",
    "best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0efe3-46fe-433b-aee3-e5fd4fc0d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = pair_segmentation(MFCC_bt,\n",
    "                         t_kernel = best_record['t_kernel'],\n",
    "                         t_stride = 1,\n",
    "                         flattened = False)\n",
    "\n",
    "best_match = torch.stack([\n",
    "    A[best_record['i']],\n",
    "    B[best_record['j']],\n",
    "])\n",
    "\n",
    "print('-' * 38 + ' Best Match ' + '-' * 38)\n",
    "plot(best_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa940de-8ce6-452e-a73d-baa93b06c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eb910-24e6-4dcd-a0fc-6275e777d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, dataset, _ = zip(*map(get_audio_attrs, cmu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcff60-ee33-474d-97f2-eba56aeb56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tuple(map(apply_vad, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2ae08-84ae-4c62-a8c4-cb03b9d495eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_groups = get_audio_groups(metadata, dataset)\n",
    "pairs = get_matching_pairs(metadata, dataset, audio_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5cb7d-6110-435f-939e-6c633a03aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = display_random_pair(dataset, pairs)\n",
    "\n",
    "# i, j = [???, ???]\n",
    "# display_pair(dataset, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ceb0bf-0da4-4ce5-93e9-19c8e3cee7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "WIN_MS, HOP_MS   = 15, 5\n",
    "WIN_LEN, HOP_LEN = map(lambda time_ms: time_ms * SAMPLERATE // 1000, [WIN_MS, HOP_MS])\n",
    "\n",
    "SCALE_RANGE      = np.arange(0.20, 1.00, 0.01)\n",
    "T_KERNEL_RANGE   = np.arange(10, 100, 1)\n",
    "T_STRIDE         = 1\n",
    "\n",
    "X_t = pad_sequence([dataset[i], dataset[j]]).T\n",
    "\n",
    "mfcc_transformer = MFCCTransformer(samplerate  = SAMPLERATE,\n",
    "                                   n_mfcc      = 13,\n",
    "                                   winlen      = WIN_LEN,\n",
    "                                   hoplen      = HOP_LEN,\n",
    "                                   alpha       = 0.97,\n",
    "                                   L           = 55,\n",
    "                                   N           = 2,\n",
    "                                   dd          = True,\n",
    "                                   rasta       = True,\n",
    "                                   postprocess = True)\n",
    "\n",
    "MFCC_bt = mfcc_transformer(X_t)\n",
    "plot(MFCC_bt, 'viridis')\n",
    "scaler = CompositeScaler()\n",
    "matching_records = matching_frames(MFCC_bt, SCALE_RANGE, T_KERNEL_RANGE, T_STRIDE)\n",
    "best_record = matching_records.iloc[0].to_dict()\n",
    "print(best_record)\n",
    "\n",
    "A, B = pair_segmentation(MFCC_bt, best_record['t_kernel'], T_STRIDE, flattened=False)\n",
    "best_match = torch.stack([\n",
    "    A[best_record['i']],\n",
    "    B[best_record['j']],\n",
    "])\n",
    "print('-' * 38 + ' Best Match ' + '-' * 38)\n",
    "plot(best_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ee10c-5ba7-4a3a-b109-55c73edc718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, a, t_kernel, i, j, _, _ = best_record.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5c8fc-7020-403f-99c9-f85ee974f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.ab_subsampling_map.loc[scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae44614-be70-40f2-b998-a459444d6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69ff84-507c-41a5-b569-9a74b9bbbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_i0, original_j0 = scaler.original_indices(i, j, scale, a)\n",
    "original_i1, original_j1 = scaler.original_indices(i + t_kernel, j + t_kernel, scale, a)\n",
    "island_a, island_b = slice(original_i0, original_i1), slice(original_j0, original_j1)\n",
    "island_a, island_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516dd0a-0f6f-4d84-8046-3e87c2065a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = Spectrogram(X_t)\n",
    "\n",
    "samples_a = InvSpectrogram(X_ft[0, :, island_a].unsqueeze(0))\n",
    "audio_a = Audio(samples_a, rate=SAMPLERATE)\n",
    "display(audio_a)\n",
    "\n",
    "samples_b = InvSpectrogram(X_ft[1, :, island_b].unsqueeze(0))\n",
    "audio_b = Audio(samples_b, rate=SAMPLERATE)\n",
    "display(audio_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b47ec-3b98-4db5-90fc-1138ff4910ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft[0, :, island_a].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5aa85-eef6-4296-9d57-220001f3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "389/403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfacb48e-5a52-4e00-8d1b-d55aa288ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10846b49-c2e3-45d3-b65f-c731a8db0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_records.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f4af7-8782-4566-af7e-f283dc0d38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "while n < matching_records.shape[0]:\n",
    "    \n",
    "    rejected_indices = []\n",
    "    best_scale, best_a, best_t_kernel, best_offset_i, best_offset_j, _, _ = matching_records.iloc[n]\n",
    "    best_offset_i, best_offset_j = scaler.original_indices(best_offset_i, best_offset_j, best_scale, best_a)\n",
    "    best_end_i, best_end_j       = scaler.original_indices(best_offset_i + best_t_kernel, \n",
    "                                                           best_offset_j + best_t_kernel, \n",
    "                                                           best_scale, best_a)\n",
    "    for m in range(n+1, matching_records.shape[0]):\n",
    "        scale, a, t_kernel, offset_i, offset_j, _, _ = matching_records.iloc[m]\n",
    "        offset_i, offset_j = scaler.original_indices(offset_i, offset_j, scale, a)\n",
    "        end_i, end_j       = scaler.original_indices(offset_i + t_kernel, \n",
    "                                                     offset_j + t_kernel, \n",
    "                                                     scale, a)\n",
    "        if best_offset_i <= offset_i <= best_end_i or \\\n",
    "           best_offset_j <= offset_j <= best_end_j or \\\n",
    "           best_offset_i <= end_i <= best_end_i or \\\n",
    "           best_offset_j <= end_j <= best_end_j:\n",
    "            rejected_indices.append(m)\n",
    "\n",
    "    matching_records.drop(rejected_indices, inplace=True)\n",
    "    n += 1\n",
    "\n",
    "len(matching_records), len(rejected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54aaa9d-4994-4bf2-bbc0-5fbdf248adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9261c-be31-4e4e-a785-e2f4f1f74499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8a6f5-bf67-4b77-81d8-1701c1ac7a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a139d2-1b00-4a47-a169-f0abd56e20f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc131b9-296f-47c8-889b-52d75030a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "kernel = 40\n",
    "stride = 1\n",
    "\n",
    "f_bins, t_bins = MFCC_bt.shape[1:]\n",
    "segments = F.unfold(\n",
    "    PdB_ft.unsqueeze(1),\n",
    "    kernel_size = (f_bins, kernel),\n",
    "    stride = (f_bins, stride)\n",
    ").transpose(2,1).reshape(2, -1, f_bins, kernel)\n",
    "seg_A, seg_B = segments.flatten(2)\n",
    "cross_corr   = (seg_A @ seg_B.T) / seg_A.shape[-1] ** 2\n",
    "plt.imshow(cross_corr, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab812aa-a2d4-4bde-80b4-2c45f8c9fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.unfold(PdB_ft.flatten(1), kernel_size=40, stride=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b6b76-a467-4f66-ba78-fe3eb0a15cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "time_bins = PdB_ft.shape[-1]\n",
    "frame_bins = 40\n",
    "\n",
    "PdB_ft = F.pad(PdB_ft, pad=(0, frame_bins - time_bins % frame_bins), value=PdB_ft.min())\n",
    "PdB_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc255bdc-83de-449c-a18c-8bbe46ea539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = torch.split(PdB_ft.unsqueeze(1), frame_bins, -1)\n",
    "segments_A, segments_B = torch.concat(segments, 1).flatten(2)\n",
    "segments_A.shape, segments_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835464c8-72fb-45ab-a2d9-f1cec7f7995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_bins = segments_A.shape[-1]\n",
    "similarity  = (segments_A @ segments_B.T) / vector_bins ** 2\n",
    "plt.imshow(similarity, origin='lower', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f980147-9621-4bfd-a90d-532fecb83a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = similarity.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42023e3-fec1-4719-b109-ed273a096b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b38f2c-12d6-46ff-8822-2c341ea424f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pad = lambda samples: F.pad(samples, pad=(0, MAX_SAMPLES - len(samples)), value=samples.abs().min())\n",
    "X_t = torch.stack([pad(dataset[idx]) for idx in (i, j)])\n",
    "original_samples = max(dataset[idx].numel() for idx in (i, j))\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7d24d-c37e-4136-848a-8565a7ff0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = Spectrogram(X_t)\n",
    "A_ft, phi_ft = map(lambda func: func(X_ft), [torch.abs, torch.angle])\n",
    "X_ft = A_ft * torch.exp(1j * phi_ft)\n",
    "X_t  = InvSpectrogram(X_ft)\n",
    "display(Audio(X_t[0], rate=SAMPLERATE))\n",
    "display(Audio(X_t[1], rate=SAMPLERATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f4ab7-3a6c-4418-8c8e-08dd52c51c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PdB_ft = 10 * torch.log10(A_ft + 1e-6)\n",
    "PdB_ft = PdB_ft.detach().clone().numpy()\n",
    "_, axs = plt.subplots(2, 1, figsize=(10, 4))\n",
    "axs[0].imshow(PdB_ft[0], origin='lower')\n",
    "axs[1].imshow(PdB_ft[1], origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5cf9a-32ae-448b-abbf-b54c727f041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mask = PdB_ft > PdB_ft.min((1,2)).reshape(-1,1,1)\n",
    "mask[:,-1,:].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037eb4e-8d94-474a-8cd3-6b31461bbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = \n",
    "hop   \n",
    "= HOP_MS * SAMPLERATE // 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39fc193-a559-443e-9b56-d72534321f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Stop here!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c14a97-4eec-4473-8db3-6356d4b4bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PdB_ft = 10 * torch.log10(A_ft + 1e-6)\n",
    "images = PdB_ft.detach().clone().numpy()\n",
    "_, axs = plt.subplots(2, 1, figsize=(10, 4))\n",
    "axs[0].imshow(images[0], origin='lower')\n",
    "axs[1].imshow(images[1], origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912ed51-0432-4e6b-9485-02206d05f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_images = images > 1\n",
    "for idx in range(2):\n",
    "    bin_images[idx] = closing(bin_images[idx], footprint=footprint_rectangle([5,5]))\n",
    "    bin_images[idx] = opening(bin_images[idx], footprint=footprint_rectangle([5,5]))\n",
    "    plt.imshow(bin_images[idx, :150, :750], origin='lower', cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6971770-c321-4358-8366-216f8092c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idx_mask = np.where(bin_images[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c13a8c-e683-49a5-9733-61511966eb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a53c00-ef3d-4300-a45c-8d050b0d64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = bin_images[:, 0]\n",
    "masks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
